{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Natarajan Krishnaswami](mailto:natarajan@krishnaswami.org)  \n",
    "W261 Machine Learning At Scale  \n",
    "Spring 2016 / Section 2  \n",
    "Homework 1, Week 2 \n",
    "17 Jan 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1.0.0.\n",
    "\n",
    "Define big data. Provide an example of a big data problem in your domain of expertise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Big data problems are where data is too copious (*volume*), rapidly arriving or changing (*velocity*), or nonuniform (*variety*) to be processed effectively on a single or small number of machines.\n",
    "\n",
    "In financial information processing, data can come in realtime from funds, index providers, exchanges, customer, etc., in a multitude of formats. These can be price or price-derived indicators (columns), corporate fundamental data scraped from disclosures, or bond/derivative characteristics scraped from contractual terms and conditions.  These all need to be normalized, identified, and stored efficiently (columnar stores are an excellent fit for historical time-series, eg).\n",
    "\n",
    "In addition to the usual three V's, data quality (*veracity*) is critical to having a trustworthy product.  Some of the ways this affects system design incude, for the slower-updating streams like index open/close values, staging/versioning to permit sanity checks (human and automated). For equities, prior revisions of estimates (e.g., analyst target for 52 week earnings) are retained to form a kind of spiky or branching time series.  (These two differ in that the former are corrections, and the latter updates; usually people are not as interested in incorrect data, except to explain errors in downstream processing.) For any data type, if erroneous values were calculated or acquired, we must be able to revise them and then regenerate consistent derived data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1.0.1.\n",
    "\n",
    "In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: I would start by plotting the data and the fit curves for each degree (with dimensionality reduced appropriately, if needed) to see if I can get an rough sense of the likely bias.\n",
    "\n",
    "One first idea could be to estimate the variance using resampled versions of the training dataset; we expect this to increase with the polynomial degree.  We expect to see the error due to squared bias decrease with polynomial degree, but since we don't know the true values, we can do pretty well just by keeping the irreducible error  and bias combined.\n",
    "\n",
    "An easier way to approach this is to note that the tradeoff corresponds precisely to generalizability, so reserving some of the data for testing trained models (or better, doing so repeatedly as in N-fold cross validation) will let us calculate a plausible stand-in for test MSE, provided we have enough data:\n",
    "\n",
    "Error in the training data suggests the extent of error due to squared bias and irreducible error, and the magnitude of increase in error between training and test data sets suggests the extent of error due to variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions/Goals\n",
    "The data you will use is a curated subset of the Enron email corpus\n",
    "(whose details you may find in the file `enronemail_README.txt` \n",
    "in the directory surrounding these instructions).\n",
    "\n",
    "In this directory you will also find starter code (`pNaiveBayes.sh`),\n",
    "(similar to the `pGrepCount.sh` code that was presented in this weeks lectures),\n",
    "which will be used as control script to a python mapper and reducer \n",
    "that you will supply at several stages. Doing some exploratory data analysis you will see (with this very small dataset) the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-01-18 19:36:02--  https://www.dropbox.com/sh/jylzkmauxkostck/AAAHAYB6SvwiGpMtJu_04mYaa/pNaiveBayes.sh?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 108.160.172.206, 108.160.172.238\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|108.160.172.206|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: https://dl.dropboxusercontent.com/content_link/Gyz69W1UoLSu4ZDTioeqCJ0R6vINxJ8GJG6BPoa8KfMRi5nR7Kx9XNOOXCwfqokE/file [following]\n",
      "--2016-01-18 19:36:02--  https://dl.dropboxusercontent.com/content_link/Gyz69W1UoLSu4ZDTioeqCJ0R6vINxJ8GJG6BPoa8KfMRi5nR7Kx9XNOOXCwfqokE/file\n",
      "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 45.58.74.5\n",
      "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.74.5|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2070 (2.0K) [text/x-sh]\n",
      "Saving to: ‘pNaiveBayes.sh’\n",
      "\n",
      "pNaiveBayes.sh      100%[=====================>]   2.02K  --.-KB/s   in 0s     \n",
      "\n",
      "2016-01-18 19:36:03 (221 MB/s) - ‘pNaiveBayes.sh’ saved [2070/2070]\n",
      "\n",
      "--2016-01-18 19:36:03--  https://www.dropbox.com/sh/jylzkmauxkostck/AAC_6JZH7yqMcxfEGPc4-_xJa/enronemail_1h.txt?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 108.160.172.238, 108.160.172.206\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|108.160.172.238|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: https://dl.dropboxusercontent.com/content_link/3a0H2eWISZrsgsXLRzcjhcLrtw6GYVfm4HyquJmXiujfYQkf1v0Bfrw5vUBhd1dw/file [following]\n",
      "--2016-01-18 19:36:04--  https://dl.dropboxusercontent.com/content_link/3a0H2eWISZrsgsXLRzcjhcLrtw6GYVfm4HyquJmXiujfYQkf1v0Bfrw5vUBhd1dw/file\n",
      "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 108.160.173.5\n",
      "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|108.160.173.5|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 204579 (200K) [text/plain]\n",
      "Saving to: ‘enronemail_1h.txt’\n",
      "\n",
      "enronemail_1h.txt   100%[=====================>] 199.78K  --.-KB/s   in 0.05s  \n",
      "\n",
      "2016-01-18 19:36:04 (3.62 MB/s) - ‘enronemail_1h.txt’ saved [204579/204579]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O pNaiveBayes.sh 'https://www.dropbox.com/sh/jylzkmauxkostck/AAAHAYB6SvwiGpMtJu_04mYaa/pNaiveBayes.sh?dl=0'\n",
    "!wget -O enronemail_1h.txt 'https://www.dropbox.com/sh/jylzkmauxkostck/AAC_6JZH7yqMcxfEGPc4-_xJa/enronemail_1h.txt?dl=0'\n",
    "# some cleaning\n",
    "import os\n",
    "import re\n",
    "with open('enronemail_1h.txt.new','w') as out:\n",
    "    with open('enronemail_1h.txt','rU') as f:\n",
    "        for line in f:\n",
    "            # fix line termination\n",
    "            line=line.strip()\n",
    "            # fix line 4\n",
    "            line=re.sub('^0001.2000-06-06.lokay\\t0\\t\" key dates and impact of upcoming sap implementation over',\n",
    "                        '0001.2000-06-06.lokay\\t0\\tkey dates and impact of upcoming sap implementation\\t\" over',\n",
    "                        line)\n",
    "            # fix line 50\n",
    "            line=re.sub('^0009.2001-06-26.SA_and_HP\\t1\\t\"double',\n",
    "                        '0009.2001-06-26.SA_and_HP\\t1\\tNA\\t\"double',\n",
    "                        line)\n",
    "            # print emits a newline\n",
    "            print >>out, line\n",
    "os.rename('enronemail_1h.txt.new', 'enronemail_1h.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ wc -l enronemail_1h.txt\n",
      "100 enronemail_1h.txt\n",
      "+ cut -f2 -d\t enronemail_1h.txt\n",
      "+ wc\n",
      "    100     100     200\n",
      "+ cut -f2 -d\t enronemail_1h.txt\n",
      "+ head\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "+ head -n 100 enronemail_1h.txt\n",
      "+ tail -1\n",
      "0018.2003-12-18.GP\t1\t await your response\t\" dear partner,  we are a team of government officials that belong to an eight-man committee in the presidential cabinet as well as the senate.  at the moment, we will be requiring your assistance in a matter that involves investment of monies, which we intend to transfer to your account, upon clarification and a workable agreement reached in consummating the project with you. based on a recommendation from an associate concerning your integrity, loyalty and understanding, we deemed it necessary to contact you accordingly. all arrangements in relation to this investment initiative, as well as the initial capital for its take off has been tactically set aside to commence whatever business you deemed fit, that will turn around profit favourably. we request you immediately contact us if you will be favorably disposed to act as a partner in this venture, and possibly will afford us the opportunity to discuss whatever proposal you may come up with. also  bear in mind that the initial capital that we shall send across will not exceed$ 13,731, 000,00 usd (thirteen million seven hundred and thirty one thousand united states dollars) so whatever areas of investment your proposal shall cover, please it should be within the set aside capital. in this regard, the proposal you may wish to discuss with us should be comprehensive enough for our better understanding; with special emphasis on the following:  1. the tax obligationin your country  2. the initial capital base required in your proposed  investment area, as well as;  3. the legal technicalities in setting up a  business in your country with foreigners as share-holders  4. the most convenient and secured mode of receiving the funds without our direct involvement.  5. your ability to provide a beneficiary/partnership account with a minimal deposit, where we shall transfer the funds into subsequently.  another area that we wish to explicitly throw more light on, is the process we have conceived in transferring the funds into the account you shall be providing. since we are the owners of the funds, and the money will be leaving the apex bank of my country, we shall purposefully fulfill the legal obligations precedent to transferring  such huge amount of funds, without arousing suspicion from any quarter as a drug or terrorist related funds; and this will assist us in the long run to forestall any form of investigations. remember that, on no account must we be seen or perceived to be directly connected with the transfer of funds. you will be the one to be doing all these, and in the course of transfer, if for any reason whatsoever, you incurred some bills, we shall adequately retire same, upon the successful confirmation of the funds in your account. the commencement of this project is based on your ability to convince us of the need to invest in whatever business you have chosen, and to trust your personality and status, especially as it concerns the security of the  funds in your custody.  i await your response,  sincerely,  john adams  (chairman senate committee on banks and currency)  call number: 234-802-306-8507 \"\n"
     ]
    }
   ],
   "source": [
    "!set -x; wc -l enronemail_1h.txt  #100 email records\n",
    "!set -x; cut -f2 -d'\t' enronemail_1h.txt|wc  #extract second field which is SPAM flag\n",
    "!set -x; cut -f2 -d'\t' enronemail_1h.txt|head\n",
    "!set -x; head -n 100 enronemail_1h.txt|tail -1  #an example SPAM email record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1.1.\n",
    "Read through the provided control script (pNaiveBayes.sh)\n",
    "   and all of its comments. When you are comfortable with their\n",
    "   purpose and function, respond to the remaining homework questions below.\n",
    "\n",
    "   A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here. (dont forget to include the Question Number and the quesition in the cell as a multiline comment!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Done.  \n",
    "N.b., instead of including the problem numbers as comments in the code cells, I've formatted this notebook to use the question numbers as headings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1.2.\n",
    "Provide a mapper/reducer pair that, when executed by `pNaiveBayes.sh`  \n",
    "   will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.\n",
    "\n",
    "To do so, make sure that\n",
    "   \n",
    "* mapper.py counts all occurrences of a single word, and\n",
    "* reducer.py collates the counts of the single word.\n",
    "\n",
    "CROSSCHECK:\n",
    "\n",
    "````bash\n",
    "$ grep assistance enronemail_1h.txt|cut -d\"`printf '\\t'`\" -f4| grep assistance|wc -l\n",
    "8````\n",
    "       \n",
    "\"assistance\" occurs on 8 lines but how many times does the token occur? 10 times! This is the number we are looking for!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "import re\n",
    "import sys\n",
    "\n",
    "if len(sys.argv) != 3:\n",
    "    print >>sys.stderr, \"Usage: {0} [filename] [word]\".format(sys.argv[0])\n",
    "    sys.exit(1)\n",
    "count=0\n",
    "linenum=0\n",
    "str_re=re.compile(r'\\w+')\n",
    "with open(sys.argv[1]) as f:\n",
    "    tgt=sys.argv[2].lower()\n",
    "    for linenum, line in enumerate(f,1):\n",
    "        fields=line.lower().split('\\t')\n",
    "        if len(fields) != 4:\n",
    "            print >>sys.stderr, \"Line {0}: expected 4 fields, found {1}\".format(linenum, len(fields))\n",
    "            continue\n",
    "        for word in str_re.findall(''.join(fields[-2:])):\n",
    "            if word == tgt:\n",
    "                count += 1\n",
    "print tgt, count\n",
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "from collections import Counter\n",
    "import sys\n",
    "\n",
    "if len(sys.argv) == 1:\n",
    "    print >>sys.stderr, \"Usage: {0} [filenames]*\".format(sys.argv[0])\n",
    "    sys.exit(1)\n",
    "# Counter to avoid ugly key existence check in the loop\n",
    "# This is overkill here, but needed if the reducers could count multiple words\n",
    "counts = Counter()\n",
    "for part_file in sys.argv[1:]:\n",
    "    with open(part_file) as f:\n",
    "        for linenum, line in enumerate(f,1):\n",
    "            words=line.strip().split()\n",
    "            counts[words[0]] += int(words[1])\n",
    "for word in sorted(counts):\n",
    "    print >>sys.stderr, '{0}\\t{1}'.format(word, counts[word])\n",
    "    print '{0}\\t{1}'.format(word, counts[word])\n",
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\n",
      "10\n",
      "copyright\t7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "!chmod +x mapper.py reducer.py pNaiveBayes.sh\n",
    "!./pNaiveBayes.sh 1 assistance\n",
    "!grep -i assistance enronemail_1h.txt | cut -d\"`printf '\\t'`\" -f3,4 | tr ' ' '\\n' | grep -i assistance | wc -l\n",
    "!./pNaiveBayes.sh 1 copyright\n",
    "!grep -i copyright enronemail_1h.txt | cut -d\"`printf '\\t'`\" -f3,4 | tr ' ' '\\n' | grep -i copyright | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Addendum**: The data errors described below were fixed in updated data for the assigment\n",
    "\n",
    "**Note**: There were some malformed lines (too few fields) in the file, which I corrected manually. I added diagnostic code to print the line numbers after the initial run threw an exception at `fields[-2:]` for a line with a single column.\n",
    "\n",
    "**Line 4**: The subject and body were concatenated (space instead of tab).  I changed a space to a tab in a reasonable spot to look like a subject and body:  \n",
    "> `key dates and impact of upcoming sap implementation[\t]over the next few weeks`\n",
    "    \n",
    "**Line 50**: The subject and body were concatenated (space instead of tab).  I didn't see a great spot to break the two, so I added a dummy subject:\n",
    ">`[???\t]double your life` \n",
    "\n",
    "**Line 60**: A word in the body was replaced by a newline.  I joined this and the subsequent line using a placeholder for the missing word:  \n",
    "> `every person was running for his life. my [???] and i managed to escape to south africa`\n",
    "\n",
    "Since the current assignment weighs subject and body equally, and only word characters are considered, the first two do not alter the probabilities.  The third is a trickier choice since the obvious approaches (concatenating the line with the prior line vs omitting the row) yield different probabilities for the words in the fragmentary line. However, it seems extremely likely to be part of the prior line.  In more ambiguous cases (especially with larger amounts of data available), it may be a better choice to omit such lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1.3.\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh  \n",
    "   will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results.\n",
    "   \n",
    "To do so, make sure that\n",
    "   \n",
    "* mapper.py and\n",
    "* reducer.py \n",
    "\n",
    "that performs a single word Naive Bayes classification. For multinomial Naive Bayes,  $\\mathrm{Pr}(X=\\mathrm{“assistance”} | Y=\\mathrm{SPAM})$ is calculated as follows:\n",
    "\n",
    "   $\\frac{\\mbox{the number of times “assistance” occurs in SPAM labeled documents}}{\\mbox{the number of words in documents labeled SPAM}}$\n",
    "\n",
    "**NOTE**: if  “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeld as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial estimate of the class conditional for a Naive Bayes Classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import re\n",
    "import sys\n",
    "\n",
    "strip_re=re.compile(r'\\W+')\n",
    "split_re=re.compile(r'''(?:\\s|\\.|[-/:,' \"@\\|+*]|\\\\)+''')\n",
    "def tokenize(*fields):\n",
    "    for field in fields:\n",
    "        if field.strip() != 'NA':\n",
    "            field=re.sub(r\"'ll\\b\", \" will\", field)\n",
    "            field=re.sub(r\"n't\\b\", \" not\", field)\n",
    "            field=re.sub(r\"'re\\b\", \" are\", field)\n",
    "            field=re.sub(r\"\\bit's\\b\", \"it is\", field)\n",
    "            for word in split_re.split(field.lower()):\n",
    "                word=strip_re.sub('', word)\n",
    "                if word:\n",
    "                    yield word\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) != 3:\n",
    "        print >>sys.stderr, \"Usage: {0} [filename] [words|*]\".format(sys.argv[0])\n",
    "        sys.exit(1)\n",
    "    with open(sys.argv[1]) as f:\n",
    "        tgts={x for x in sys.argv[2].lower().split()}\n",
    "        for linenum, line in enumerate(f):\n",
    "            fields=line.split('\\t')\n",
    "            if len(fields) != 4:\n",
    "                print >>sys.stderr, \"Line {0}: expected 4 fields, found {1}\".format(linenum, len(fields))\n",
    "                continue\n",
    "            docid=fields[0]\n",
    "            classid=('spam' if fields[1] == '1' else 'ham')\n",
    "            words=Counter(tokenize(fields[2], fields[3]))\n",
    "            print 'doc {0} {1} {2}'.format(docid, classid, sum(words.values()))\n",
    "            for word, count in words.items():\n",
    "                tgt=(word in tgts or '*' in tgts)\n",
    "                print 'term {0} {1} {2} {3} {4}'.format(docid, classid, word, count, tgt)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "from collections import namedtuple, defaultdict, Counter\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "class NBClassifier(object):\n",
    "    \"\"\"Class to consume output of mapper.py to train a multinomial\n",
    "    naive Bayes classifier, and to predict classes.\n",
    "    \"\"\"\n",
    "    # little helpers to refer to mapper output fields by name instead of position\n",
    "    DocRecord=namedtuple(\"DocRecord\",['docid', 'classid', 'count'])\n",
    "    TermRecord=namedtuple(\"TermRecord\",['docid', 'classid', 'term', 'count', 'target'])\n",
    "\n",
    "    def __init__(self, alpha=0.0):\n",
    "        \"\"\"Create a naive bayes classifier instance with the specified smoothing:\n",
    "        0 for no smoothing\n",
    "        1 for Laplace smoothing\n",
    "        Any other value for Lidstone smoothing\n",
    "        \"\"\"\n",
    "        self.alpha=alpha\n",
    "        self.verbose=False\n",
    "        self.class_counts=Counter()\n",
    "        self.doc_classes={}\n",
    "        self.doc_term_counts=defaultdict(lambda:defaultdict(lambda:0))\n",
    "        self.class_term_counts=defaultdict(lambda:Counter())\n",
    "        self.class_tot_counts=Counter()\n",
    "        self.class_priors=None\n",
    "        self.cond_probs=None\n",
    "        self.targets=set()\n",
    "        \n",
    "    def fit(self, iterable):\n",
    "        \"\"\"Consume mapper records and calculate probabilities\"\"\"\n",
    "        for line in iterable:\n",
    "            self._add_rec(line)\n",
    "        self._calc_priors()\n",
    "        self._calc_cond_probs()\n",
    "    def predict_proba(self, doc_term_counts):\n",
    "        \"\"\"Return class posterior probs of a list of term counts, omitting untargeted terms\"\"\"\n",
    "        posteriors={}\n",
    "        used={}\n",
    "        for classid, term_probs in self.cond_probs.items():\n",
    "            posteriors[classid]=self.class_priors[classid]\n",
    "\n",
    "        classid=''\n",
    "        term=''\n",
    "        for classid, denom in self.class_tot_counts.items():\n",
    "            try:\n",
    "                for term, doc_count in doc_term_counts.items():\n",
    "                    if term in self.targets:\n",
    "                        n_terms=len(self.class_term_counts[classid])\n",
    "                        term_count=self.class_term_counts[classid][term]\n",
    "                        cond_prob=math.log(term_count+self.alpha)-math.log(denom+self.alpha*n_terms)\n",
    "                        posteriors[classid]+=doc_count*cond_prob\n",
    "            except ValueError:\n",
    "                del(posteriors[classid])\n",
    "                if self.verbose:\n",
    "                    print '{0} not present in class {1} term list'.format(term, classid)\n",
    "                pass\n",
    "        return posteriors\n",
    "    \n",
    "    def predict(self, doc_term_counts):\n",
    "        \"\"\"Return class ID maximizing posterior prob of a list of term counts, omitting untargeted terms\"\"\"\n",
    "        posteriors=self.predict_proba(doc_term_counts)\n",
    "        return max(posteriors, key=lambda x: posteriors[x])\n",
    "\n",
    "    \n",
    "    def _add_rec(self, line):\n",
    "        \"\"\"Consume mapper-produced doc and term records\"\"\"\n",
    "        try:\n",
    "            args=line.strip().split()\n",
    "            if args[0]=='doc':\n",
    "                if self.verbose:\n",
    "                    print >>sys.stderr,\"Processing doc\", args[1]\n",
    "                rec=NBClassifier.DocRecord(args[1], args[2], int(args[3]))\n",
    "                self.doc_classes[rec.docid]=rec.classid\n",
    "                self.class_counts[rec.classid]+=1\n",
    "            elif args[0]=='term':\n",
    "                rec=NBClassifier.TermRecord(args[1], args[2], args[3], float(args[4]), args[5])\n",
    "                self.class_term_counts[rec.classid][rec.term]+=rec.count\n",
    "                self.class_tot_counts[rec.classid]+=rec.count\n",
    "                self.doc_term_counts[rec.docid][rec.term]+=rec.count\n",
    "                if rec.target == 'True':\n",
    "                    self.targets.add(rec.term)\n",
    "            else:\n",
    "                print >>sys.stderr, 'Unexpected row: {0}'.format(line)\n",
    "        except ValueError, n:\n",
    "            print >>sys.stderr, line\n",
    "            raise n\n",
    "    def _calc_priors(self):\n",
    "        \"\"\"Calculate log prior probs for a doc to be in each class\"\"\"\n",
    "        doc_count=sum(self.class_counts.values())\n",
    "        self.class_priors=defaultdict(lambda:0)\n",
    "        for classid, count in self.class_counts.items():\n",
    "            self.class_priors[classid]=math.log(1.0*count/doc_count)\n",
    "\n",
    "    def _calc_cond_probs(self):\n",
    "        self.cond_probs=defaultdict(lambda:defaultdict(lambda:0))\n",
    "        for classid, denom in self.class_tot_counts.items():\n",
    "            n_terms=len(self.class_term_counts[classid])\n",
    "            for term, count in self.class_term_counts[classid].items():\n",
    "                self.cond_probs[classid][term]=math.log(count+self.alpha)-math.log(denom+self.alpha*n_terms)\n",
    "\n",
    "def gen_lines(files):\n",
    "    \"\"\"The moral equivalent of\n",
    "       `itertools.chain(open(fn) for fn in files)`\n",
    "    but only opening one file at a time and ensuring `close()`\n",
    "    gets called appropriately\n",
    "    \"\"\"\n",
    "    for part_file in files:\n",
    "        with open(part_file) as f:\n",
    "            for line in f:\n",
    "                yield line\n",
    "def main():\n",
    "    if len(sys.argv) == 1:\n",
    "        print >>sys.stderr, \"Usage: {0} [filenames]*\".format(sys.argv[0])\n",
    "        sys.exit(1)\n",
    "    # Make a classifier object\n",
    "    alpha=os.environ.get('alpha',0.0)\n",
    "    if isinstance(alpha, basestring):\n",
    "        try:\n",
    "            alpha=float(alpha)\n",
    "        except:\n",
    "            alpha=1.0\n",
    "    \n",
    "    nbc=NBClassifier(alpha=alpha)\n",
    "    nbc.fit(gen_lines(sys.argv[1:]))\n",
    "    nbc.verbose='verbose' in os.environ\n",
    "    \n",
    "    print >>sys.stderr, \"Processed {0} records\".format(len(nbc.doc_classes))\n",
    "    print >>sys.stderr, \"Classes:\"\n",
    "    for classid in nbc.class_counts:\n",
    "        print >>sys.stderr, \"    {0: <4.4s}: {1} terms, {2} docs, {3:.3f} prior\".format(\n",
    "            classid,\n",
    "            len(nbc.cond_probs[classid]),\n",
    "            nbc.class_counts[classid],\n",
    "            nbc.class_priors[classid],\n",
    "        )\n",
    "        if nbc.verbose:\n",
    "            for term, cond in itertools.islice(sorted(nbc.cond_probs[classid].items(),key=lambda x: -x[1]),10):\n",
    "                print >>sys.stderr, \"        {0}:\\t{1}\".format(term, cond)\n",
    "    \n",
    "    preds=[]\n",
    "    matches=[]\n",
    "    for docid, actual in nbc.doc_classes.items():\n",
    "        pred=nbc.predict(nbc.doc_term_counts[docid])\n",
    "        preds.append(pred)\n",
    "        matches.append(pred==actual)\n",
    "        print '{0}\\t{1:d}\\t{2:d}'.format(docid, actual=='spam', pred=='spam')\n",
    "        if nbc.verbose:\n",
    "            probs=nbc.predict_proba(nbc.doc_term_counts[docid])\n",
    "            print >>sys.stderr, '{0: <20.20s}  pred: {1: <4.4}  match: {2:1d}  {3}'.format(\n",
    "                docid, pred, matches[-1],\n",
    "            ', '.join('{0}: {1:.2f}'.format(k,v) for k,v in probs.items()))\n",
    "    print >>sys.stderr, \"Matches:\",sum(matches),'of',len(matches)\n",
    "    print >>sys.stderr, \"Accuracy:\",1.0*sum(matches)/len(matches)\n",
    "    counts=defaultdict(lambda:0.0)\n",
    "    for pred, match in zip(preds, matches):\n",
    "        tag=''\n",
    "        if match:\n",
    "            tag += 't'\n",
    "        else:\n",
    "            tag += 'f'\n",
    "        if pred=='spam':\n",
    "            tag += 'p'\n",
    "        else:\n",
    "            tag += 'n'\n",
    "        counts[tag] += 1\n",
    "    for k,v in counts.items():\n",
    "        print >>sys.stderr, \"{0}: {1}\".format(k,v)\n",
    "    if counts['tp']+counts['fp'] > 0:\n",
    "        print >>sys.stderr, \"Precision: {0:.3f}\".format(\n",
    "            counts['tp']/(counts['tp']+counts['fp']))\n",
    "    if counts['tp']+counts['fn'] > 0:\n",
    "        print >>sys.stderr, \"Recall:    {0:<.3f}\".format(\n",
    "            counts['tp']/(counts['tp']+counts['fn']))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 records\r\n",
      "Classes:\r\n",
      "    ham : 2724 terms, 56 docs, -0.580 prior\r\n",
      "    spam: 3736 terms, 44 docs, -0.821 prior\r\n",
      "Matches: 56 of 100\r\n",
      "Accuracy: 0.56\r\n",
      "tn: 56.0\r\n",
      "fn: 44.0\r\n",
      "Recall:    0.000\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x *.py\n",
    "!./pNaiveBayes.sh 1 banananana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When only targeting a word not present in the dataset, we see that as expected the priors win: the classifer predicts everything to be ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 records\r\n",
      "Classes:\r\n",
      "    ham : 2724 terms, 56 docs, -0.580 prior\r\n",
      "    spam: 3736 terms, 44 docs, -0.821 prior\r\n",
      "Matches: 60 of 100\r\n",
      "Accuracy: 0.6\r\n",
      "tn: 54.0\r\n",
      "fp: 2.0\r\n",
      "tp: 6.0\r\n",
      "fn: 38.0\r\n",
      "Precision: 0.750\r\n",
      "Recall:    0.136\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 1 assistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the term 'assistance' changes 8 of these predictions to spam.  This is wrong for 2 emails, but correct for 6 others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1.4.\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh  \n",
    "   will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results\n",
    "   \n",
    "To do so, make sure that\n",
    "\n",
    "* mapper.py counts all occurrences of a list of words, and\n",
    "* reducer.py \n",
    "\n",
    "performs the multiple-word Naive Bayes classification via the chosen list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 records\r\n",
      "Classes:\r\n",
      "    ham : 2724 terms, 56 docs, -0.580 prior\r\n",
      "    spam: 3736 terms, 44 docs, -0.821 prior\r\n",
      "Matches: 63 of 100\r\n",
      "Accuracy: 0.63\r\n",
      "tn: 54.0\r\n",
      "fp: 2.0\r\n",
      "tp: 9.0\r\n",
      "fn: 35.0\r\n",
      "Precision: 0.818\r\n",
      "Recall:    0.205\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 1 'assistance valium enlargementWithATypo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `valium` is not present in the hams, this didn't help any.  With smoothing, we see more benefit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 records\r\n",
      "Classes:\r\n",
      "    ham : 2724 terms, 56 docs, -0.580 prior\r\n",
      "    spam: 3736 terms, 44 docs, -0.821 prior\r\n",
      "Matches: 63 of 100\r\n",
      "Accuracy: 0.63\r\n",
      "tn: 54.0\r\n",
      "fp: 2.0\r\n",
      "tp: 9.0\r\n",
      "fn: 35.0\r\n",
      "Precision: 0.818\r\n",
      "Recall:    0.205\r\n"
     ]
    }
   ],
   "source": [
    "!alpha=1 ./pNaiveBayes.sh 1 'assistance valium enlargementWithATypo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1.5.\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh  \n",
    "will classify the email messages by all words present.\n",
    "\n",
    "To do so, make sure that\n",
    "\n",
    "* mapper.py counts all occurrences of all words, and\n",
    "* reducer.py performs a word-distribution-wide Naive Bayes classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 records\n",
      "Classes:\n",
      "    ham : 2724 terms, 56 docs, -0.580 prior\n",
      "    spam: 3736 terms, 44 docs, -0.821 prior\n",
      "Matches: 100 of 100\n",
      "Accuracy: 1.0\n",
      "tn: 56.0\n",
      "tp: 44.0\n",
      "Precision: 1.000\n",
      "Recall:    1.000\n"
     ]
    }
   ],
   "source": [
    "!alpha=0 ./pNaiveBayes.sh 1 '*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 records\n",
      "Classes:\n",
      "    ham : 2724 terms, 56 docs, -0.580 prior\n",
      "    spam: 3736 terms, 44 docs, -0.821 prior\n",
      "Matches: 100 of 100\n",
      "Accuracy: 1.0\n",
      "tn: 56.0\n",
      "tp: 44.0\n",
      "Precision: 1.000\n",
      "Recall:    1.000\n"
     ]
    }
   ],
   "source": [
    "!alpha=1 ./pNaiveBayes.sh 1 '*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**: Even the tiniest bit of smoothing appears to be required to avoid blowing up by trying to take the log of zero. Once that is added, the classifier predicts correctly for all the training data.\n",
    "\n",
    "**Addendum**: I modified the classifier to reject classes that trigger $\\mathrm{log}(0)$ thus:\n",
    "\n",
    "````python\n",
    "del posteriors[classid]\n",
    "````\n",
    "\n",
    "After which, the unsmoothed case performs perfectly as well. Running with verbose flag enabled confirms there is always at least one word present in each document that is present in only the ham or the spam term lists.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all cases, mapper.py will read in a portion of the email data,\n",
    "count some words and print out counts to a file.\n",
    "\n",
    "While the utility of the reducer will change significantly \n",
    "across steps, it will always be responsible for reading in \n",
    "counts of words and collating data.\n",
    "\n",
    "In all cases you should apply a Laplace (add-1) smoothing to the classifier\n",
    "(always on the reducer side) to safeguard code against low-data.\n",
    "\n",
    "You will find in the starter code (pNaiveBayes.sh) that the basic\n",
    "operations (e.g., splitting the original data, scheduling the mappers, \n",
    "waiting, running the reducer, and cleaning up the intermediate data files)\n",
    "are taken care of, and that the portion of this assignment left for you\n",
    "is in python and will involve regular expressions, counting with objects, \n",
    "and some light math.\n",
    "\n",
    "For a quick reference on the construction of the classifier that you will code,\n",
    "please consult the \"Document Classification\" section of the following wikipedia page:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Document_classification\n",
    "\n",
    "the original paper by our curators of the Enron email data:\n",
    "\n",
    "http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf\n",
    "\n",
    "or the recording of this week's live lecture that you will find on the LMS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1.6.\n",
    "Benchmark your code with the Python SciKit-Learn implementation of multinomial Naive Bayes.\n",
    "\n",
    "It always a good idea to test your solutions against publicly available libraries such as SciKit-Learn, The Machine Learning toolkit available in Python. In this exercise, we benchmark ourselves against the SciKit-Learn implementation of multinomial Naive Bayes.  For more information on this implementation see: http://scikit-learn.org/stable/modules/naive_bayes.html more  \n",
    "\n",
    "Let's define  Training error = misclassification rate with respect to a training set. It is more formally defined here:\n",
    "\n",
    "Let $DF$ represent the training set in the following:  \n",
    "$$\\mathrm{Err}(\\mathrm{Model}, DF) = \\frac{\\lvert\\{(X, c(X)) ∈ DF : c(X) \\neq \\mathrm{Model}(x)\\}\\rvert}{\\lvert DF\\rvert}$$\n",
    "\n",
    "Where $\\lvert\\rvert$ denotes set cardinality; $c(X)$ denotes the class of the tuple $X$ in $DF$; and $\\mathrm{Model}(X)$ denotes the class inferred by the model $\\mathrm{Model}$\n",
    "\n",
    "In this exercise, please complete the following:\n",
    "\n",
    "1. Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW1.5 and report the Training error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "2. Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW1.5 and report the Training error \n",
    "3. Run the Multinomial Naive Bayes algorithm you developed for HW1.5 over the same data used HW1.5 and report the Training error \n",
    "4. Please prepare a table to present your results\n",
    "5. Explain/justify any differences in terms of training error rates over the dataset in HW1.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn (Hint: smoothing, which we will discuss in next lecture)\n",
    "6. Discuss the performance differences in terms of training error rates over the dataset in HW1.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1.6.1: Quick and easy ingest using pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def read_data(fn):\n",
    "    data=pd.read_csv(fn, sep='\\t', header=None, na_values=['NA'])\n",
    "    data.columns=['id','spam', 'subject', 'body']\n",
    "    data.index=data['id']\n",
    "    data = data.replace(np.nan,'')\n",
    "    data['doc']=data['subject']+' '+data['body']\n",
    "    return data[['spam','doc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>doc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0001.1999-12-10.farmer</th>\n",
       "      <td>0</td>\n",
       "      <td>christmas tree farm pictures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001.1999-12-10.kaminski</th>\n",
       "      <td>0</td>\n",
       "      <td>re: rankings  thank you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001.2000-01-17.beck</th>\n",
       "      <td>0</td>\n",
       "      <td>leadership development pilot  sally:  what ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001.2000-06-06.lokay</th>\n",
       "      <td>0</td>\n",
       "      <td>key dates and impact of upcoming sap implement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001.2001-02-07.kitchen</th>\n",
       "      <td>0</td>\n",
       "      <td>key hr issues going forward  a) year end revi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001.2001-04-02.williams</th>\n",
       "      <td>0</td>\n",
       "      <td>re: quasi  good morning,  i'd love to go get ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0002.1999-12-13.farmer</th>\n",
       "      <td>0</td>\n",
       "      <td>vastar resources, inc.  gary, production from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0002.2001-02-07.kitchen</th>\n",
       "      <td>0</td>\n",
       "      <td>congrats!  contratulations on the execution o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0002.2001-05-25.SA_and_HP</th>\n",
       "      <td>1</td>\n",
       "      <td>fw: this is the solution i mentioned lsc  oo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0002.2003-12-18.GP</th>\n",
       "      <td>1</td>\n",
       "      <td>adv: space saving computer to replace that bi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           spam  \\\n",
       "id                                \n",
       "0001.1999-12-10.farmer        0   \n",
       "0001.1999-12-10.kaminski      0   \n",
       "0001.2000-01-17.beck          0   \n",
       "0001.2000-06-06.lokay         0   \n",
       "0001.2001-02-07.kitchen       0   \n",
       "0001.2001-04-02.williams      0   \n",
       "0002.1999-12-13.farmer        0   \n",
       "0002.2001-02-07.kitchen       0   \n",
       "0002.2001-05-25.SA_and_HP     1   \n",
       "0002.2003-12-18.GP            1   \n",
       "\n",
       "                                                                         doc  \n",
       "id                                                                            \n",
       "0001.1999-12-10.farmer                         christmas tree farm pictures   \n",
       "0001.1999-12-10.kaminski                            re: rankings  thank you.  \n",
       "0001.2000-01-17.beck        leadership development pilot  sally:  what ti...  \n",
       "0001.2000-06-06.lokay      key dates and impact of upcoming sap implement...  \n",
       "0001.2001-02-07.kitchen     key hr issues going forward  a) year end revi...  \n",
       "0001.2001-04-02.williams    re: quasi  good morning,  i'd love to go get ...  \n",
       "0002.1999-12-13.farmer      vastar resources, inc.  gary, production from...  \n",
       "0002.2001-02-07.kitchen     congrats!  contratulations on the execution o...  \n",
       "0002.2001-05-25.SA_and_HP   fw: this is the solution i mentioned lsc  oo ...  \n",
       "0002.2003-12-18.GP          adv: space saving computer to replace that bi...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=read_data(\"enronemail_1h.txt\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1.6.1 copied to use the same tokenizer between sklearn and hand-written NB classifiers\n",
    "import re\n",
    "\n",
    "strip_re=re.compile(r'\\W+')\n",
    "split_re=re.compile(r'''(?:\\s|\\.|[-/:,' \"@\\|+*]|\\\\)+''')\n",
    "def tokenize(*fields):\n",
    "    for field in fields:\n",
    "        if field.strip() != 'NA':\n",
    "            field=re.sub(r\"'ll\\b\", \" will\", field)\n",
    "            field=re.sub(r\"n't\\b\", \" not\", field)\n",
    "            field=re.sub(r\"'re\\b\", \" are\", field)\n",
    "            field=re.sub(r\"\\bit's\\b\", \"it is\", field)\n",
    "            for word in split_re.split(field.lower()):\n",
    "                word=strip_re.sub('', word)\n",
    "                if word:\n",
    "                    yield word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def hw16_1(data, alpha):\n",
    "    cv=CountVectorizer(tokenizer=tokenize)\n",
    "    mnb=MultinomialNB(alpha=alpha)\n",
    "    pl=Pipeline([('cv',cv),('mnb',mnb)])\n",
    "    pl.fit(data['doc'],data['spam'])\n",
    "    pred=pl.predict(data['doc'])\n",
    "    print \"alpha={0:.3f}\".format(alpha)\n",
    "    print \"Training_error={0:.3f}\".format(1-accuracy_score(data['spam'], pred))\n",
    "    print classification_report(data['spam'], pred, target_names=['ham', 'spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0.000\n",
      "Training_error=0.000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       1.00      1.00      1.00        56\n",
      "       spam       1.00      1.00      1.00        44\n",
      "\n",
      "avg / total       1.00      1.00      1.00       100\n",
      "\n",
      "alpha=1.000\n",
      "Training_error=0.000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       1.00      1.00      1.00        56\n",
      "       spam       1.00      1.00      1.00        44\n",
      "\n",
      "avg / total       1.00      1.00      1.00       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hw16_1(read_data(\"enronemail_1h.txt\"),0)\n",
    "hw16_1(read_data(\"enronemail_1h.txt\"),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hw16_2(data, alpha):\n",
    "    cv=CountVectorizer(tokenizer=tokenize)\n",
    "    bnb=BernoulliNB(alpha=alpha,binarize=1)\n",
    "    pl=Pipeline([('cv',cv),('bnb',bnb)])\n",
    "    pl.fit(data['doc'],data['spam'])\n",
    "    pred=pl.predict(data['doc'])\n",
    "    print \"alpha={0:.3f}\".format(alpha)\n",
    "    print \"Training_error={0:.3f}\".format(1-accuracy_score(data['spam'], pred))\n",
    "    print classification_report(data['spam'], pred, target_names=['ham', 'spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0.000\n",
      "Training_error=0.060\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.90      1.00      0.95        56\n",
      "       spam       1.00      0.86      0.93        44\n",
      "\n",
      "avg / total       0.95      0.94      0.94       100\n",
      "\n",
      "alpha=0.500\n",
      "Training_error=0.290\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.66      1.00      0.79        56\n",
      "       spam       1.00      0.34      0.51        44\n",
      "\n",
      "avg / total       0.81      0.71      0.67       100\n",
      "\n",
      "alpha=1.000\n",
      "Training_error=0.340\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.62      1.00      0.77        56\n",
      "       spam       1.00      0.23      0.37        44\n",
      "\n",
      "avg / total       0.79      0.66      0.59       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hw16_2(read_data(\"enronemail_1h.txt\"),0.0)\n",
    "hw16_2(read_data(\"enronemail_1h.txt\"),0.5)\n",
    "hw16_2(read_data(\"enronemail_1h.txt\"),1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6.3\n",
    "Training error as defined above is $(1-\\mathrm{Accuracy}(\\mathrm{Model}, DF))$; so the classifier in problem 1.5 has a training error of 0 or 1, depending on whether smoothing is enabled or not, per the accuracy figure logged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6.4 Training error table\n",
    "<style type='text/css'>\n",
    "tr:nth-child(odd)  { background-color:#eee; }\n",
    "tr:nth-child(even) { background-color:#fff; }\n",
    "</style>\n",
    "\n",
    "|   |Model|$\\alpha$|Training error|\n",
    "|---|-----|--------|--------------|\n",
    "|1|sklearn Multinomial NB|0.00|0.00|\n",
    "|2|sklearn Multinomial NB|1.00|0.00|\n",
    "|3|sklearn Bernoulli NB|0.00|0.06|\n",
    "|4|sklearn Bernoulli NB|0.50|0.29|\n",
    "|5|sklearn Bernoulli NB|1.00|0.34|\n",
    "|6|Prob 1.5 Multinomial NB|0.00|0.00|\n",
    "|7|Prob 1.5 Multinomial NB|1.00|0.00|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6.5 Discussion\n",
    "The main oddity in the results above is how the problem 1.5 classifier exploded without smoothing.  I would not expect *every* mail to include a term present exclusively in the ham or spam term lists, but that seems to be the only reason every classification would fail without smoothing. I adjust details of how the invalid operation is avoided above, and my classifer correctly classifies each document.  When weighting is enabled, rather than outright rejecting the class, the class conditional (log) probability takes the form:\n",
    "\n",
    "$$d\\,\\mathrm{log}\\left(\\alpha\\right) - d\\,\\mathrm{log}\\left(\\alpha\\cdot \\lvert T_C\\rvert + \\sum_{t \\in T_C} n_t\\right)$$\n",
    "\n",
    "where $T_C$ is the set of terms in the class, $n_t$ is the total number of times term $t$ occurs in the class, and $d$ is the number of times the out-of-vocabulary term occurs in the document.  This corresponds to a term that appears $\\alpha$ times in the class, so smoothing values between 0 to 1 make missing terms behave like they are present but less frequent than any other term in the vocabulary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
