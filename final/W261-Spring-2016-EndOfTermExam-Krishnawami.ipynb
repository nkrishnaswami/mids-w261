{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIDS Machine Learning at Scale  \n",
    "End of Term exam  \n",
    "Week 15\n",
    "Spring, 2016  \n",
    "[Natarajan Krishnaswami](natarajan@krishnaswami.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam Schedule  (All times are in California Time)\n",
    "\n",
    "4:00 PM - 6:00 PM \n",
    "\n",
    "Exam location is at:\n",
    "\n",
    "    https://www.dropbox.com/s/k9mw3rnr86iktk6/MIDS-MLS-End-of-Term-2016-Spring-2016-04-27-Published.txt?dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions for exam \n",
    "\n",
    "Instructions:\n",
    " \n",
    "> Within 2 hours of receiving my email can you  email me with two attachments:  \n",
    "> * ipython notebook that you use for calculations\n",
    "> * your end of term responses using  the following format. \n",
    "> \n",
    "> ```\n",
    "> ET1:a,c,\n",
    "> ET2:d\n",
    "> ET3:c\n",
    "> ..... \n",
    "> ```\n",
    "> \n",
    "> And complete exam submission form (http://goo.gl/forms/ggNYfRXz0t).\n",
    "> \n",
    "> Good luck,\n",
    "> Jimi\n",
    "\n",
    "1. Please acknowledge receipt of exam by sending a quick reply to the instructors\n",
    "2. Review the submission form first to scope it out (it will take a 5-10 minutes to input your \n",
    "   answers and other information into this form)\n",
    "3. Please keep all your work and responses in ONE (1) notebook only (and submit via the form)\n",
    "4. Please make sure that the NBViewer link for your Submission notebook works\n",
    "5. Please do NOT discuss this exam with anyone (including your class mates) until after 8AM (West coast time) Friday, April 29, 2016 \n",
    "6. This is a take home exam. Please complete by yourself without assistance from others.\n",
    "\n",
    "Please use your live session time from week 8 to complete this mid term \n",
    "(plus an additional 30 minutes if you need it). \n",
    "This is an open book exam meaning you can consult webpages and textbooks \n",
    "(but not each other or other people). Please complete this exam by yourself.\n",
    "\n",
    "Please submit your solutions and notebook via the following form:\n",
    "\n",
    "      http://goo.gl/forms/ggNYfRXz0t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam questions begins here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:1\n",
    "Using one-hot-encoding, a categorical feature with four distinct values would be represented by how many features?\n",
    "\n",
    "(a) 1 feature  \n",
    "(b) 2 features  \n",
    "(c) 4 features  \n",
    "(d) none of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "c, 4 features.  \n",
    "In OHE, each category is represented by a 0/1 value.  Unlike dummy coding, all zeroes does not represent a valid state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:2\n",
    "In the following (and also referring to HW12: http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/1wb2rdqbet54y1h/MIDS-MLS-Project-Criteo-CTR.ipynb) we have hashed the three sample points using numBuckets=4 and numBuckets=100. Complete the three statements below about these hashed features summarized in the following  table using each answer once.\n",
    "\n",
    "|Name|Raw Features|4 Buckets|100 Buckets|\n",
    "|--|--|--|--|\n",
    "|sampleOne|[(0, 'mouse'), (1, 'black')]|{2: 1.0, 3: 1.0}|{14: 1.0, 31: 1.0}|\n",
    "|sampleTwo|[(0, 'cat'), (1, 'tabby'), (2, 'mouse')]|{0: 2.0, 2: 1.0}|{40: 1.0, 16: 1.0, 62: 1.0}|\n",
    "|sampleThree|[(0, 'bear'), (1, 'black'), (2, 'salmon')|{0: 1.0, 1: 1.0, 2: 1.0}|{72: 1.0, 5: 1.0, 14: 1.0}|\n",
    "\n",
    "With 100 buckets, sampleOne and sampleThree both contain index 14 due to __________.\n",
    "\n",
    "(a) A hash collision   \n",
    "(b) Underlying properties of the data   \n",
    "(c) The fact that used 100 buckets   \n",
    "(d) none of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: b, Underlying properties of the data.  \n",
    "Both samples contain the term `black`, which must hash to the same value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:3\n",
    "In the following (and also referring to HW12: http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/1wb2rdqbet54y1h/MIDS-MLS-Project-Criteo-CTR.ipynb) we have hashed the three sample points using numBuckets=4 and numBuckets=100. Complete the three statements below about these hashed features summarized in the following table using each answer once.\n",
    "\n",
    "|Name|Raw Features|4 Buckets|100 Buckets|\n",
    "|--|--|--|--|\n",
    "|sampleOne|[(0, 'mouse'), (1, 'black')]|{2: 1.0, 3: 1.0}|{14: 1.0, 31: 1.0}|\n",
    "|sampleTwo|[(0, 'cat'), (1, 'tabby'), (2, 'mouse')]|{0: 2.0, 2: 1.0}|{40: 1.0, 16: 1.0, 62: 1.0}|\n",
    "|sampleThree|[(0, 'bear'), (1, 'black'), (2, 'salmon')|{0: 1.0, 1: 1.0, 2: 1.0}|{72: 1.0, 5: 1.0, 14: 1.0}|\n",
    "\n",
    "It is likely that sampleTwo has two indices with 4 buckets, but three indices with 100 buckets due to __________.\n",
    "\n",
    "(a) A hash collision  \n",
    "(b) Underlying properties of the data  \n",
    "(c) The fact that we go from 4 to 100 buckets  \n",
    "(d) none of the above  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: c, The fact that we go from 4 to 100 buckets.  \n",
    "The sample has three terms, so it will yield fewer than 3 indices only if the hash function yields the same value, or if it yields values that are the same modulo the bucket count. It will yield 2 indices for 4 buckets and 3 for 100 buckets only if the hash value for the colliding item does not have a hash code $N$ s.t. $N\\, \\mathrm{mod}\\, 100 = N\\, \\mathrm{mod}\\ 4$ (For example, $N$ and $N+100$ would collide for both choices of bucket count).\n",
    "\n",
    "*Note*: I typically refer to to entire calculation yielding a bucket index as the hash function, but interpret this question as distinguishing between the common, bucket-size-independent portion of the hash function and the assignment to buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:4\n",
    "In the following (and also referring to HW12: http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/1wb2rdqbet54y1h/MIDS-MLS-Project-Criteo-CTR.ipynb) we have hashed the three sample points using numBuckets=4 and numBuckets=100. Complete the three statements below about these hashed features summarized in the following table using each answer once.\n",
    "\n",
    "|Name|Raw Features|4 Buckets|100 Buckets|\n",
    "|--|--|--|--|\n",
    "|sampleOne|[(0, 'mouse'), (1, 'black')]|{2: 1.0, 3: 1.0}|{14: 1.0, 31: 1.0}|\n",
    "|sampleTwo|[(0, 'cat'), (1, 'tabby'), (2, 'mouse')]|{0: 2.0, 2: 1.0}|{40: 1.0, 16: 1.0, 62: 1.0}|\n",
    "|sampleThree|[(0, 'bear'), (1, 'black'), (2, 'salmon')|{0: 1.0, 1: 1.0, 2: 1.0}|{72: 1.0, 5: 1.0, 14: 1.0}|\n",
    "\n",
    "With 4 buckets, sampleTwo and sampleThree both contain index 0 due to __________.\n",
    "\n",
    "(a) A hash collision  \n",
    "(b) Underlying properties of the data  \n",
    "(c) The fact that we use 4 buckets  \n",
    "(d) none of the above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: a, A hash collision. This is actually still identical to problem 3: some pair of distinct terms map to the same index with 4 buckets, but to different indices with 100 buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:5  When applying numerical machine learning approaches (and for non-numerical approaches if required) to big data problems which of the following steps are could be used during modeling and are recommended:\n",
    "\n",
    "(a) Convert categorical features to numerical features via one-hot-encoding and store in a dense representation  \n",
    "(b) Transform  categorical features using hashing regardless of how many unique categorical values exist in training and test data  \n",
    "(c) Use matrix factorization to remap your input vectors to latent concepts  \n",
    "(d) none of the above  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: a,b,c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:6\n",
    "When dealing with numercial data which of the following are ways to deal with missing data:\n",
    "\n",
    "(a) Delete records that have missing input values  \n",
    "(b) Standardize the data and set all missing values to 1 (one)  \n",
    "(c) Use K-nearest neighbours based on the test set to fill in missing values in the training set  \n",
    "(d) none of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: a, Delete records that have missing input values.  \n",
    "After standardizing, one should impute with 0s (mean).  \n",
    "It would take some care to impute using nearest neighbors (as a regression -- mean of $k$ neighbors), but it is possible.  Similarly, low rank matrix approximation could be used for imputation.  However, one should absolutely not use test data to impute training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:7\n",
    "In the Criteo project, we're trying to predict what:\n",
    "\n",
    "(a) Revenue from click events  \n",
    "(b) Click-through vs not click event  \n",
    "(c) Probability of a click event  \n",
    "(d) none of the above  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: b, Click-through vs not click event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:8\n",
    "Which of the following are true about the purpose of a loss function?\n",
    "\n",
    "(a) Itâ€™s a way to penalize a model for incorrect predictions  \n",
    "(b) It precisely defines the optimization problem to be solved for a particular learning model  \n",
    "(c) Loss functions can be used for modeling both classification and regression problems  \n",
    "(d) none of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: a,b,c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:9\n",
    "When implementing Logistic Regression with Regularization in Spark which of the following apply\n",
    "\n",
    "(a) When lambda equals one, it provides the same result as standard logistic regression   \n",
    "(b) One only needs to modify the standard logistic regression by modifying the Mapper    \n",
    "(c) Can be framed as minimizing a convex function  \n",
    "(d) none of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: b,c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:10\n",
    "In the context of ecommerce you have just deployed a new conversion rate prediction model to production. This model (aka treatment model) will challenge the control nodel (i.e., the current model) in AB Test manner to see if it can be produce better revenue. Here is the data that was taken from this live AB Test. \n",
    "\n",
    "```\n",
    "CONTROL MODEL (our new CTR model)\n",
    "Impression ID   Revenue  \n",
    "1                $0.50\n",
    "2                $0.50\n",
    "3                $3.00\n",
    "......               \n",
    "20000            $3.00\n",
    "20001            $3.00\n",
    "20002            $3.00\n",
    "20003            $3.00\n",
    "......\n",
    "50,001           $3.00\n",
    ".....\n",
    "100,000          $4.00\n",
    "```\n",
    "\n",
    "All other impressions in this 100,000 sample resulted in zero transactions and therefore zero revenue. \n",
    "\n",
    "```\n",
    "TREATMENT MODEL (our new CTR model)\n",
    "Impression ID   Revenue  \n",
    "1                $1.50\n",
    "2                $0.50\n",
    "3                $0.00\n",
    "......\n",
    "50,001           $3.00\n",
    ".....\n",
    "100,000          $4.00\n",
    "```\n",
    "All other impressions in this 100,000 sample resulted in zero transactions and therefore zero revenue. \n",
    "\n",
    "\n",
    "P-values are a common way to determine the statistical significance of a test. The smaller it is, the more confident you can be that the test results are due to something other than random chance.\n",
    "A common p-value of .05 is a 5% significance level. Similarly, a p-value of .01 is a 1% significance level. A p-value of .20 is a 20% significance level. For this problem set the p-value to 0.01\n",
    "\n",
    "\n",
    "Which of the following are true:\n",
    "\n",
    "(a) Based on revenue there is no statistical significant difference between the Control and the Treatment at p-value of 0.05 for a one-sided t-test  \n",
    "(b) Based on transaction rates (tranactions that generated revenue versus not) there is no statistical significant difference between the Control and the Treatment at p-value of 0.05 for a one-sided t-test  \n",
    "(c) AB testing using differences in revenue for this problem is a  useful means of determining if the Treatment conversion rate prediction model is better than the control model.  \n",
    "(d) none of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array(2.7393492169536784), 0.0061566398816400225),\n",
       " (array(3.0001200083894655), 0.0026990648571502914))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "import scipy.stats\n",
    "\n",
    "control=[.5, .5, 3, 3, 3, 3, 3, 3, 4]\n",
    "control.extend([0]*(100000-len(control)))\n",
    "control=np.asarray(control)\n",
    "\n",
    "treatment=[ 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "treatment.extend([0]*(100000-len(treatment)))\n",
    "treatment=np.asarray(treatment)\n",
    "\n",
    "#nb 2-sided test\n",
    "scipy.stats.ttest_ind(control, treatment), scipy.stats.ttest_ind(control.astype(bool), treatment.astype(bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:11\n",
    "Given this graph expressed in the form of an adjacency list,\n",
    "\n",
    "```\n",
    "Node  adjacentNode:weightAssociatedWithEdge\n",
    "N1    N6:10,  N2:2\n",
    "N2    N3:1\n",
    "N3    N4:1\n",
    "N4    N5:1\n",
    "N5    N6:1\n",
    "N6    N7:1\n",
    "N7    N8:1\n",
    "N8    N9:1\n",
    "```\n",
    "\n",
    "Using the parallel breadth-first search algorithm for determining the shortest path from a single source, how many iterations are required to discover the shortest distances to all nodes from Node 1 \n",
    "\n",
    "A 7   \n",
    "B 8    \n",
    "C 13  \n",
    "D None of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: b, 8 rounds  \n",
    "Path through 2 is the shortest, so every node must be traversed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:12 When parallelizing support vector machines and related algorithms in map-reduce frameworks, which of the following statements are true:\n",
    "\n",
    "(a) In the context of support vector machines, nonlinear kernels such as quadratic kernels can be readily parallelized in map reduce frameworks such as Spark  \n",
    "(b) In the context of support vector machines, linear kernels can be readily parallelized in map reduce frameworks such as Spark  \n",
    "(c) Sequential learning via algorithms such perceptron can take advantage of map-reduce frameworks and yield the same results as a single core implementation with significant reductions in training time  \n",
    "(d) Field-aware Factorization Machines can easily distributed to take advantage of map-reduce frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: a,b,d  \n",
    "Not perceptron, but the others are amenable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:13 Given the following paired RDDs \n",
    "RDD1 = {(1, 2), (3, 4), (3, 6)}\n",
    "RDD2 = {(3, 9) (3, 6)}\n",
    "\n",
    "Using PySpark, write code to perform an inner join of these paired RDDs. What is the resulting RDD? Make your Spark available in your notebook:\n",
    "\n",
    "A: [(3, (4, 9)), (3, (6, 9))]\n",
    "B: [(3, (4, 9)), (3, (4, 6)), (3, (6, 9)), (3, (6, 6))]\n",
    "C: [(3, (4, 9)), (3, (4, 6)), (3, (6, 9)), (3, (6, 9))]\n",
    "D: None of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, (6, 9)), (3, (6, 6)), (3, (4, 9)), (3, (4, 6))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1=sc.parallelize([(1, 2), (3, 4), (3, 6)])\n",
    "r2=sc.parallelize([(3, 9),(3, 6)])\n",
    "r1.join(r2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:14  You have been tasked to build a predictive model to forecast beer sales for a chain of stores.\n",
    "After doing basic exploratory analysis on the data, what is the first thing you do regarding modeling?\n",
    "\n",
    "\n",
    "(a) Construct a baseline model  \n",
    "(b) Determine a metric to evaluate your machine learnt models  \n",
    "(c) Split your data into training, validation and test subsets (or split using cross fold validatation)  \n",
    "(d) All of the  of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: d. With the caveat that I would split out (sample) the data prior to exploration, c needs to occur temporally before model training, but is really part of that process (esp with CV or $\\varepsilon$-differential privacy for training data generation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:15 \n",
    "Use Spark and the following notebook, https://www.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb?dl=0 to answer this question.\n",
    "\n",
    "The mean absolute percentage error (MAPE), also known as mean absolute percentage deviation (MAPD), is a measure of prediction accuracy of a model for say a forecasting method in statistics, \n",
    "for example in trend estimation. It usually expresses accuracy as a percentage, and is defined by the formula:\n",
    "\n",
    "MAPE = average over all examples (100*Abs(Actual - Predicted) / Actual)) \n",
    "\n",
    "Note when Actual is zero that test row is dropped from the evaluation.\n",
    "\n",
    "Construct a mean model for target variable `CASES18PK`. Calculate the MAPE for the mean model over the training set. Select the closest answer.\n",
    "\n",
    "(a) 200%  \n",
    "(b) 250%  \n",
    "(c) 20%  \n",
    "(d) 180%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Week', u'PRICE12PK', u'PRICE18PK', u'PRICE30PK', u'CASES12PK', u'CASES18PK', u'CASES30PK']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel\n",
    "import os\n",
    "df=sc.textFile(\n",
    "    \"beerSales.txt\"\n",
    ")\n",
    "print df.take(1)[0].split('\\t')\n",
    "df=df.zipWithIndex().filter(\n",
    "    lambda x: x[1]>0\n",
    ").map(\n",
    "    lambda x: [float(y) for y in x[0].split('\\t')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def doReg(data):\n",
    "    lr=LinearRegressionWithSGD.train(data, iterations=10000, step=0.0001)\n",
    "    val_pred=data.map(lambda pt: (pt.label, lr.predict(pt.features)))\n",
    "    MAPE = val_pred.map(\n",
    "        lambda (v, p): 100*abs(v - p)/v\n",
    "    ).reduce(\n",
    "        lambda x, y: x + y) / val_pred.count()\n",
    "    print(\"Mean Abs Pred Error = \" + str(MAPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Abs Pred Error = 171.737094196\n"
     ]
    }
   ],
   "source": [
    "doReg(df.map(lambda x: LabeledPoint(x[5],x[1:4])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer**: d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:16\n",
    "Use Spark and the following notebook, https://www.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb?dl=0 to answer this question.\n",
    "\n",
    "The target variable `CASES18P`K is skewed, so take the log of it (and make it more normally distributed) and compute the MAPE of the mean model for `CASES18PK`. Select the closest answer to your calculated MAPE.\n",
    "\n",
    "(a) 200%  \n",
    "(b) 30%  \n",
    "(c) 20%  \n",
    "(d) 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Abs Pred Error = 37.4346369254\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "doReg(df.map(lambda x: LabeledPoint(math.log(x[5]),x[1:4])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:17\n",
    "Use Spark and the following notebook, https://www.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb?dl=0 to answer this question.\n",
    "\n",
    "Build a linear regression model using the following variables:\n",
    "\n",
    "Log(CASES18PK)  ~  log(PRICE12PK), \tlog(PRICE18PK),\tlog(PRICE30PK)\n",
    "\n",
    "Calculate MAPE over the test set and select the closest answer.\n",
    "\n",
    "(a) 4.3%\n",
    "(b) 4.6%\n",
    "(c) 3.5%\n",
    "(d) 3.9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Abs Pred Error = 99.1063425636\n"
     ]
    }
   ],
   "source": [
    "doReg(df.map(lambda x: LabeledPoint(math.log(x[5]),\n",
    "                                    [math.log(xx) for xx in x[1:4]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET:18\n",
    "Recall that Spark automatically sends all variables referenced in your closures to the\n",
    "worker nodes. While this is convenient, it can also be inefficient because (1) the\n",
    "default task launching mechanism is optimized for small task sizes, and (2) you\n",
    "might, in fact, use the same variable in multiple parallel operations, but Spark will\n",
    "send it separately for each operation. As an example, say that we wanted to write a\n",
    "Spark program that looks up countries by their call signs (e.g., the call sign for Ireland is EJZ) by prefix matching in an\n",
    "table. In the following the \"signPrefixes\" variable is essentially a table with two columns \"Sign\" and \"Country Name\". The goal is \n",
    "to join the following tables:\n",
    "\n",
    "`signPrefixes` table with columns \"Sign\" and \"Country Name\"  \n",
    "`contactCounts` table with columns \"Sign\" and \"count\"\n",
    "\n",
    "to yield  a new table:\n",
    "\n",
    "`countryContactCounts` with the following columns \"Country Name\" and \"count\"\n",
    "\n",
    "Use Spark and the following notebook, https://www.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb?dl=0 to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#..... Other code...\n",
    "#Country lookup code\n",
    "\n",
    "## Helper functions for looking up the call signs\n",
    "\n",
    " def lookupCountry(sign, prefixes):\n",
    "    pos = bisect.bisect_left(prefixes, sign)\n",
    "    return prefixes[pos].split(\",\")[1]\n",
    "\n",
    "\n",
    "def loadCallSignTable():\n",
    "    f = open(\"callsign_tbl_sorted.txt\", \"r\")\n",
    "    return f.readlines()\n",
    "\n",
    "## Lookup the locations of the call signs on the\n",
    "## RDD contactCounts. We load a list of call sign\n",
    "## prefixes to country code to support this lookup.\n",
    "signPrefixes = loadCallSignTable()\n",
    "\n",
    "\n",
    "def processSignCount(sign_count, signPrefixes):\n",
    "    country = lookupCountry(sign_count[0], signPrefixes)\n",
    "    count = sign_count[1]\n",
    "    return (country, count)\n",
    "\n",
    "countryContactCounts = (contactCounts\n",
    "                        .map(lambda signCount: processSignCount(signCount, signPrefixes))\n",
    "                        .reduceByKey((lambda x, y: x + y)))\n",
    "\n",
    "countryContactCounts.saveAsTextFile(outputDir + \"/countries.txt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we modfify this code to make it more efficient? Choose one response only\n",
    "\n",
    "(a) modify line 18 with `sc.broadcast(loadCallSignTable())`\n",
    "(b) Use accumulators to store the counts for each country  \n",
    "(c) The code is already optimal  \n",
    "(d) none of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: a, modify line 18 with `sc.broadcast(loadCallSignTable())`\n",
    "This is an improvement since the table is potentially large table, and broadcast variables are distributed more efficiently (peer to peer).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
